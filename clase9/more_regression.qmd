---
  format:
    revealjs: 
      theme: [default, style.scss]
      slide-number: true
      highlight-style: github
      width: 1400
      css:
       - https://cdn.jsdelivr.net/npm/bootstrap-icons@1.8.2/font/bootstrap-icons.css
---
  
##  {.center}

<h1 class="text-indigo-pink">

Analítica de Datos

<h1>

<h2>Más aspectos sobre Regresión Lineal</h2>

::: {style="margin-top:50px"}
### Carlos Cardona Andrade {.text-orange-gold}
:::

# Tabla de contenido

1. [Condiciones del modelo](#condic)
2. [Consideraciones para una Regresión](#regresion)
3. [Transformación Logarítmica](#log)

# Condiciones del modelo {#condic .text-wash-black background="linear-gradient(45deg, #00aadd, #66dd00)"}

```{r}
#| echo: false
#| eval: true
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 4

library(tidyverse)
library(readxl)
library(janitor)
library(knitr)
library(broom)
hollywood <- read_excel("C:/Users/ccard/Downloads/KEL702-XLS-ENG.xls", sheet = "Exhibit 1")
hollywood <- hollywood %>%
  clean_names()
hollywood <- hollywood %>% 
  rename(us_gross = total_u_s_gross) %>%
  mutate(us_gross=us_gross/1000000)
hollywood <- hollywood %>% 
  rename(non_us_gross = total_non_u_s_gross) %>%
  mutate(non_us_gross=non_us_gross/1000000)
hollywood <- hollywood %>% 
  mutate(opening_gross=opening_gross/1000000)
hollywood <- hollywood %>% 
  mutate(budget=budget/1000000)

```

## Diagnóstico general de una regresión lineal {.text-lime-cyan}


```{r}
#| eval: true
#| echo: false
#| fig-align: "center"
#| out-width: "100%"
knitr::include_graphics("images/reg_intro.jpg")

```


## Condiciones del modelo {.text-lime-cyan}

::: incremental

1. `Linealidad:` hay una relación lineal entre la variable dependiente y la variable explicativa

2. `Varianza Constante:` la variabilidad de los errores es igual para todos los valores de la variable explicativa

3. `Normalidad:` los errores siguen una distribución normal

4. `Independencia:` los errores son independientes entre ellos

:::



## US Gross vs Budget {.text-lime-cyan}

```{r}
#| echo: true
#| eval: true
#| fig-align: center

ggplot(data = hollywood, aes(x = budget, y = us_gross)) +
    geom_point() +
    labs(
        x = "Budget (in millions)",
        y = "Opening Gross (in millions)"
    ) +
    theme_minimal()

```



## US Gross vs Budget {.text-lime-cyan}


$$\widehat{\text{US Total Gross}} = 18 + 0.84 \times \text{Budget}$$


```{r}
#| echo: true
#| eval: true
hollywood_model <- lm(us_gross ~ budget, data=hollywood)
tidy(hollywood_model, conf.int = TRUE)
```



## Error vs Residuo {.text-lime-cyan}

$$
Y = \beta_0 + \beta_1 X_1  + \varepsilon
$$
NUNCA vamos a conocer $\varepsilon$ pero podemos usar los residuos como un estimado de los errores. 

$$Residuo=e_i=Y_i-\hat{Y_i}$$


## ¿Son lineales los residuos? {.text-lime-cyan}

```{r}
#| echo: false
#| eval: true
#| fig-align: center

augment(hollywood_model) %>%
  ggplot(aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_smooth(se = FALSE) +
  labs(x = "Valores predichos", y = "Residuos")
```

✅Los residuos no siguen un patrón o estructura clara. Parecen aleatoriamente distribuidos

## ❌  Claro patrón en los residuos {.text-lime-cyan}


```{r}
#| eval: true
#| echo: false
#| fig-align: "center"
#| out-width: "100%"
knitr::include_graphics("images/residuals_1.png")

```


## Evaluando varianza constante {.text-lime-cyan}

```{r}
#| echo: true
#| eval: true
#| fig-align: center

augment(hollywood_model) %>%
  ggplot(aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_smooth(se = FALSE) +
  labs(x = "Valores predichos", y = "Residuos")
```

✅La dispersión vertical de los residuos es relativamente constante en la gráfica

## ❌ La varianza no es constante {.text-lime-cyan}


```{r}
#| eval: true
#| echo: false
#| fig-align: "center"
#| out-width: "100%"
knitr::include_graphics("images/residuals_2.png")

```



## Condiciones del modelo {.text-lime-cyan}

```{r}
#| echo: true
#| eval: true
#| fig-align: center

augment(hollywood_model) %>%
  ggplot(aes(x = .resid)) +
  geom_histogram() +
  labs(title = "Distribución de los residuos",
       x = "Residuos") +
  theme_minimal()
```

✅La distribución de los errores se parece a una distribución normal



## Independencia {.text-lime-cyan}

::: incremental


- Podemos verificar el supuesto de independencia a menudo basándonos en el contexto de los datos y en cómo se recolectaron las observaciones.

- Si los datos se recolectaron en un orden particular, examina un diagrama de dispersión de los residuos versus el orden en que se recolectaron los datos.

:::

:::{.fragment}

✅ Basado en la información disponible, el error de una película no nos dice nada sobre el error de otra película.

:::


## En la práctica.. {.text-lime-cyan}

Al verificar las condiciones del modelo, preguntense si alguna desviación de estas condiciones es tan grande que:

1. Se deba proponer un modelo diferente.

2. Las conclusiones extraídas del modelo deban usarse con precaución.


:::{.fragment}
Si no es así, las condiciones se cumplen suficientemente y podemos proceder con el modelo actual.
:::


# Consideraciones adicionales para una<br>[Regresión]{.hl .hl-gold} {#regresion .text-wash-black background="linear-gradient(45deg, #f37335, #fdc830)"}

## Cuarteto de Anscombe {.text-orange-gold}

```{r}
#| eval: true
#| echo: false
#| fig-align: "center"
#| out-width: "100%"
knitr::include_graphics("images/anscombe.jpg")

```

## Relaciones no lineales {.text-orange-gold}

```{r}
#| echo: false
#| eval: true
#| fig-align: center
set.seed(123)

# Create a tibble with 50 observations
galletas <- tibble(
  galletas = 1:20,
  felicidad = 2 + 0.3 * galletas - 0.02 * galletas^2 + rnorm(20, mean = 0, sd = 0.5)  # Non-linear (inverted U-shape)
)


base_cookies <- ggplot(galletas, aes(x=galletas, y=felicidad)) + 
  geom_point(size = 3) +
  coord_cartesian(xlim = c(0, 20), ylim = c(-1, 5)) +
  scale_x_continuous(breaks = 0:20) +
  labs(x = "Galletas consumidas", y = "Nivel de Felicidad") +
  theme_minimal(base_size = 14) +
  theme(panel.grid.minor = element_blank(),
        plot.title = element_text(face = "bold"))

base_cookies
```


## Relaciones no lineales {.text-orange-gold}

```{r}
#| echo: false
#| eval: true
#| fig-align: center
base_cookies +
  geom_smooth(method = "lm", color = "#0074D9", se = FALSE)
```



## Relaciones no lineales {.text-orange-gold}

```{r}
#| echo: false
#| eval: true
#| fig-align: center
base_cookies +
  geom_smooth(method = "loess", color = "#0074D9", se = FALSE)
```


## Relaciones no lineales {.text-orange-gold}

```{r}
#| echo: true
#| eval: true
#| fig-align: center
modelo_felicidad <- lm(felicidad ~ galletas + I(galletas^2), data = galletas)
tidy(modelo_felicidad, conf.int = TRUE)
```

```{r}
#| echo: true
#| eval: true
#| fig-align: center
library(knitr)
tidy(modelo_felicidad, conf.int = TRUE) %>%
  knitr::kable(format = "html") %>%
  kableExtra::kable_styling(font_size = 30)
```


## ¿Existe la relación cuadrática que vemos en la gráfica? {.text-orange-gold}

$$\widehat{Felicidad}=\hat{\beta_0}+\hat{\beta_1}\times Galletas +\hat{\beta_2}\times Galletas^2 $$


:::{.fragment}

Evaluamos con una prueba de hipótesis:

$$H_0:\hat{\beta_2}=0$$
$$H_1: \hat{\beta_2} \neq 0$$
:::

## ¿Qué hacer con una observación atípica (outlier)? {.text-orange-gold}

```{r}
#| echo: false
#| eval: true
#| fig-align: center
galletas <- tibble(
  felicidad = c(0.5, 2, 1, 2.5, 3, 1.5, 2, 2.5, 8, 3),
  galletas = 1:10
)

base_cookies <- ggplot(galletas, aes(x=galletas, y=felicidad)) + 
  geom_point(size = 3) +
  coord_cartesian(xlim = c(0, 10), ylim = c(0, 8)) +
  scale_x_continuous(breaks = 0:10) +
  labs(x = "Galletas consumidas", y = "Nivel de Felicidad") +
  theme_minimal(base_size = 14) +
  theme(panel.grid.minor = element_blank(),
        plot.title = element_text(face = "bold"))

base_cookies

```


## ¿Qué hacer con una observación atípica (outlier)? {.text-orange-gold}

```{r}
#| echo: false
#| eval: true
#| fig-align: center

# Remove the outlier where felicidad == 8
galletas_no_outlier <- galletas %>% filter(felicidad != 8)

# Combine data with an indicator for whether it's with or without the outlier
galletas$group <- "Con outlier"
galletas_no_outlier$group <- "Sin outlier"

# Combine both datasets
combined_data <- bind_rows(galletas, galletas_no_outlier)

# Base plot
base_cookies <- ggplot(combined_data, aes(x = galletas, y = felicidad, group = group)) + 
  geom_point(size = 3) +
  coord_cartesian(xlim = c(0, 10), ylim = c(0, 8)) +
  scale_x_continuous(breaks = 0:10) +
  labs(x = "Galletas consumidas", y = "Nivel de Felicidad") +
  theme_minimal(base_size = 14) +
  theme(panel.grid.minor = element_blank(),
        plot.title = element_text(face = "bold"))

# Plot with legend for solid and dashed lines
base_cookies +
  geom_smooth(aes(color = group, linetype = group), method = "lm", se = FALSE) + 
  scale_color_manual(values = c("Con outlier" = "#0074D9", "Sin outlier" = "red")) +
  scale_linetype_manual(values = c("Con outlier" = "solid", "Sin outlier" = "dashed")) +
  labs(title = "",
       color = "Regresión", linetype = "Regresión")

```


## ¿Cómo identificar los outliers? {.text-orange-gold}

No existen reglas estrictas. Depende del conocimiento del área y la comprensión de la recopilación de datos para identificar valores atípicos, inusuales e imposibles.

- Pero tenemos algunas herramientas:
    1. Histogramas
    2. Diagramas de caja
    3. Análisis de de los residuos
    

## Efecto interacción {.text-orange-gold}

Hasta ahora hemos asumido que el efecto de una variable es independiente de otra:

$$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \varepsilon
$$
El efecto del incremento en una unidad de $X_1$ en $Y$, es siempre $\beta_1$ e independiente de $X_2$


## Efecto interacción {.text-orange-gold}

Si pensamos que el efecto de $X_1$ depende del valor de $X_2$, entonces debemos agregar una tercera variable de interacción al modelo:

$$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1X_2+ \varepsilon
$$
El termino de interacción es $X_1X_2$


## Efecto interacción - Hollywood {.text-orange-gold}

El productor de Hollywood piensa que las críticas afectan menos el recaudo en US de las comedias que del resto de películas. Para eso estimamos:


```{r}
#| echo: true
#| eval: true
#| fig-align: center
hollywood <- hollywood %>%
  mutate(comedy = ifelse(genre == "Comedy", 1, 0))
model_part9<- lm(us_gross ~ opening_gross + budget + mpaa_d 
                 + comedy * critics_opinion , data=hollywood)
tidy(model_part9, conf.int = TRUE)
```



# [Transformación<br>[[Logarítmica]{.hl .hl-purple}]{style="font-size:75%;"}]{style="color:white"} {#multiple background="linear-gradient(45deg, #4a00e0, #ff0099)"}


## Transformación logarítmica {.text-indigo-pink}

Podemos considerar una transformación logarítmica de las variables de nuestro modelo (tanto variable dependiente e independientes) cuando:

::: incremental

- Exista una relación no-lineal (exponencial) entre la variable dependiente y la explicativa.

- Alguna de las variables tenga una distribución sesgada (muy distinta a la normal).
:::


## Transformación logarítmica {.text-indigo-pink}

```{r}
#| echo: false
#| eval: true
#| fig-align: center
ggplot(data = hollywood, aes(x = us_gross)) +
    geom_density() +
    labs(
        x = "US Gross (in millions)",
    ) +
    theme_minimal()
```


## Transformación logarítmica {.text-indigo-pink}

```{r}
#| echo: false
#| eval: true
#| fig-align: center
ggplot(data = hollywood, aes(x = log(us_gross))) +
    geom_density() +
    labs(
        x = "US Gross (in logs)",
    ) +
    theme_minimal()
```


## Transformación logarítmica {.text-indigo-pink}


```{r}
#| echo: true
#| eval: true

hollywood_model <- lm(us_gross ~ opening_gross, data=hollywood)
tidy(hollywood_model, conf.int = TRUE)


hollywood <- hollywood %>% 
  mutate(log_opening_gross=log(opening_gross),
         log_us_gross=log(us_gross))

hollywood_model_logs <- lm(log_us_gross ~ log_opening_gross, data=hollywood)
tidy(hollywood_model_logs, conf.int = TRUE)
```


## Regresión Nivel-Nivel {.text-indigo-pink}

$$Y=\beta_0+\beta_1X+\varepsilon$$
Interpretación: un cambio de una unidad en $X$, está asociado a un cambio en $\beta_1$ unidades en $Y$.

## Regresión Nivel-Log {.text-indigo-pink}

$$Y=\beta_0+\beta_1log(X)+\varepsilon$$
Interpretación: un aumento de 1% en $X$, es asociado a un cambio en $\beta_1/100$ unidades a $Y$.

## Regresión Log-Nivel {.text-indigo-pink}

$$log(Y)=\beta_0+\beta_1X+\varepsilon$$
Interpretación: un incremento de una unidad en $X$, está asociado a un cambio de $(\beta_1\times 100)\%$ en $Y$.

## Regresión Log-Log {.text-indigo-pink}

$$log(Y)=\beta_0+\beta_1log(X)+\varepsilon$$
Interpretación: un aumento de 1% en $X$, está asociado a un cambio de $\beta_1\%$  en $Y$.